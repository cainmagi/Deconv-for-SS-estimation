\documentclass[]{article}

\usepackage{amsmath,amssymb,amsbsy}
\usepackage[letterpaper,total={6in,8in}]{geometry}
\usepackage[colorlinks]{hyperref}

\ifpdf
  \RequirePackage{graphicx}
  \RequirePackage{epstopdf}
  \DeclareGraphicsExtensions{.pdf,.jpeg,.png,.jpg}
\else
  \RequirePackage[dvipdfmx]{graphicx}
  \RequirePackage{bmpsize}
  \DeclareGraphicsExtensions{.eps,.pdf,.jpeg,.png,.jpg}
\fi
\graphicspath{{pics/},}

\RequirePackage{algorithm}
\RequirePackage{algorithmic}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\providecommand{\algorithmautorefname}{Algorithm}

\RequirePackage{subfigure}
\RequirePackage{empheq}
\providecommand{\subfigureautorefname}{\figureautorefname}

%opening
\title{Theory Report for ``Multichannel Deconvolution of Skin Conductance Data: Concurrent Separation of Tonic and Phasic Component''}
\author{Yuchen Jin, Jin Lu}

\providecommand{\od}{\mathrm{d}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\maketitle

\section{Theory}

For any channel $n$, the skin conductance (SC) could be decomposed by a phasic component and a tonic component,
\begin{align}
  y_{\rm{SC}_n}(t) = p_n(t) + s_n(t) + \nu_n(t),
\end{align}
where the phasic component $p_n(t)$ is stimulated by the autonomic nervous system (ANS), and the tonic component $s_n(t)$ is influenced by the thermoregulation. In this work, we will model both components by different methods. The phasic component is decomposed by a series of state-space models sharing the same input, while the tonic component is described as a combination of different spline functions. Based on the theory-based multichannel forward modeling, the stimuli from the ANS could be solved by a joint optimization. In the meanwhile, the tonic component would be extracted from the data.

\subsection{Model the phasic component with a multichannel state-space representation}
According to the presumption in \cite{alexander2005separating,society2012publication,amin2019robust}, the phasic components detected in different regions of the body are regulated by the same ANS signal, the detected phasic component for channel $n$ could be formulated by
\begin{align}
  p_n(t) = \alpha_n \zeta_n (t),
\end{align}
where $\alpha_n$ denotes the attenuation caused by the the different number and sizes of sweat glands in different location. $\zeta_n(t)$ is an internal variable which is modeled by a smoothing filter applied to the neural stimuli $u(t-\beta_n)$. The conduction delay $\beta_n$ shows that the same stimuli would be detected at different moments. Generally, the neural stimuli could be defined as a time-series spike signal, i.e. $u(t) = \sum_{i=1}^N q_i \delta(t - \Delta_i)$, where $q_i$ is the spike signal in time interval i, and $\Delta_i$ is the time for ith time interval. Given $u(t)$, \textit{Alexander et al.}~\cite{alexander2005separating} provides a second-order ordinary differential equation to describe $\zeta_n(t)$,
\begin{align} \label{fml:the:ode}
  \tau_r \tau_d \frac{\od^2 \zeta_n(t)}{\od t^2} + (\tau_r + \tau_d) \frac{\od \zeta_n(t)}{\od t} + \zeta_n(t) = u(t - \beta_n).
\end{align}

The difference between different channels in \eqref{fml:the:ode} is only the time delay. In other words, the solutions of $\zeta_n(t)$ for different $n$ are the same except the time delay. To solve \eqref{fml:the:ode}, \textit{Faghih et al.}~\cite{faghih2015characterization} propose a state space model. Incorporating the time delay into the internal signal $\zeta_n(t)$, we denote another internal state $x^{(n)}_2 (t)$ as $\zeta_n(t + \beta_n)$. The state-space model could be formulated as the following form,
\begin{subequations} \label{fml:the:state-raw}
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{empheq}[left=\empheqlbrace]{align}
    \dot{x}^{(n)}_1 (t) &= a x^{(n)}_1(t) + b u(t),\\
    \tau_r \tau_d \dot{x}^{(n)}_2 (t) &= c x^{(n)}_1(t) + d x^{(n)}_2(t), \label{fml:the:state-raw-2}\\
    y_n (t) &= \alpha_n x^{(n)}_2(t).
  \end{empheq}
\end{subequations}
{\color{red}  It would be nice if there are more physiological explanation for Eq. 4.}

where $x^{(n)}_1(t)$ is another internal state. To solve the coefficients $a, b, c, d$, we eliminate $x^{(n)}_1(t)$ in \eqref{fml:the:state-raw-2},
\begin{equation} \label{fml:the:ode-from-state}
  \begin{aligned}
    \tau_r \tau_d \ddot{x}_2 (t) &= c \dot{x}^{(n)}_1(t) + d \dot{x}^{(n)}_2(t) = c \left(a x^{(n)}_1(t) + b u(t) \right) + d \dot{x}^{(n)}_2(t) \\
    &= a ( \tau_r \tau_d \dot{x}^{(n)}_2 (t) - d x^{(n)}_2(t) ) + bc u(t) + d \dot{x}^{(n)}_2(t), \\
    &= (a \tau_r \tau_d + d) \dot{x}^{(n)}_2 (t) - a d x^{(n)}_2(t) + bc u(t).
  \end{aligned}
\end{equation}

To ensure the coherence between \eqref{fml:the:ode} and \eqref{fml:the:ode-from-state}, we have,
\begin{subequations}
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{empheq}[left=\empheqlbrace]{align}
    a \tau_r \tau_d + d &= - \tau_r - \tau_d,\\
    ad &= 1, \\
    bc &= 1. 
  \end{empheq}
\end{subequations}

Given $a = -\frac{1}{\tau_r}$ and $b = \frac{1}{\tau_r}$, we could solve the above equations, and substitute the solutions into \eqref{fml:the:state-raw},
\begin{subequations} \label{fml:the:state-sing}
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{empheq}[left=\empheqlbrace]{align}
    \begin{bmatrix}
      \dot{x}^{(n)}_1 (t) \\ \dot{x}^{(n)}_2 (t)
    \end{bmatrix} &= \begin{bmatrix}
      -1/{\tau_r} & 0 \\ 1/{\tau_d} & -1/{\tau_d}
    \end{bmatrix} \begin{bmatrix}
      x^{(n)}_1(t) \\ x^{(n)}_2 (t)
    \end{bmatrix} + \begin{bmatrix}
      1/{\tau_r} \\ 0
    \end{bmatrix} u(t), \\
    y_n (t) &= \alpha_n x^{(n)}_2(t).
  \end{empheq}
\end{subequations}

Given the single-channel transmission matrix $\phi=\begin{bmatrix}
-1/{\tau_r} & 0 \\ 1/{\tau_d} & -1/{\tau_d}
\end{bmatrix}$, we could derive \eqref{fml:the:state-sing} into the multichannel form. When we have $\chi$ channels of data, the multichannel state-space model is
\begin{subequations} \label{fml:the:state-mul}
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{empheq}[left=\empheqlbrace]{align}
    \dot{\mathbf{x}} (t) &= \mathbf{A} \mathbf{x} (t) + \mathbf{B} u (t), \\
    \mathbf{y} (t) &= \mathbf{C} \mathbf{x} (t),
  \end{empheq}
\end{subequations}
where $\mathbf{x} (t) = \begin{bmatrix}
  x^{(1)}_1 (t) \\ x^{(1)}_2 (t) \\ \vdots \\ x^{(\chi)}_1 (t) \\ x^{(\chi)}_2 (t)
\end{bmatrix}$, $\mathbf{y} (t) = \begin{bmatrix}
  y_1 (t) \\ y_2 (t) \\ \vdots \\ y_{\chi} (t)
\end{bmatrix}$, $\mathbf{A} = \begin{bmatrix}
  \phi & \mathbf{0} & \cdots & \mathbf{0} \\
  \mathbf{0} & \phi & \cdots & \mathbf{0} \\
  \vdots & \vdots & \ddots & \vdots \\
  \mathbf{0} & \mathbf{0} & \cdots & \phi
\end{bmatrix}$, $\mathbf{B} = \begin{bmatrix}
  1/{\tau_r} \\ 0 \\ \vdots \\ 1/{\tau_r} \\ 0
\end{bmatrix}$, and\\$\mathbf{C} = \begin{bmatrix}
  0 & \alpha_1 & 0 & 0 & \cdots & 0 & 0 \\ 0 & 0 & 0 & \alpha_2 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\ 0 & 0 & 0 & 0 & \cdots & 0 & \alpha_{\chi}
\end{bmatrix}$.

The above model could be rewritten as a discrete model. Denote the sampling rates of the observation and the neural stimuli as $T_y$ and $T_u$ respectively, we could formulate the discretion of the time as $t_k = k T_y$ and $\Delta_i = i T_u$ for both signals. When $T_u$ is small enough, we could approximate the discrete neural stimuli by $\mathbf{u} = \begin{bmatrix}
  q_1 & q_2 & \cdots & q_N
\end{bmatrix}^T$, where we use $q_i=0$ to represent no signal at the time step $\Delta_i$. Based on these configurations, we could derive the discrete model matrices by
\begin{subequations} \label{fml:the:state-dis-mat}
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{empheq}[left=\empheqlbrace]{align}
    \mathcal{A} &= \mathcal{L}^{-1}\{ s \mathbf{I} - \mathbf{A} \}(T_u), \\
    \mathcal{B} &= \mathbf{A}^{-1}(\mathcal{A} - \mathbf{I})\mathbf{B}, \\
    \mathcal{C} &= \mathbf{C},
  \end{empheq}
\end{subequations}
where $\mathcal{L}^{-1}$ is the inverse Laplace transform.
{\color{red}	Please check Eq. 9. The equation does not look right.}

The discrete form of \eqref{fml:the:state-mul} is
\begin{subequations} 
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{empheq}[left=\empheqlbrace]{align}
    \mathbf{x} [k+1] &= \mathcal{A} \mathbf{x} [k] + \mathcal{B} \mathbf{u} [k], \\
    \mathbf{y} [k] &= \mathcal{C} \mathbf{x} [k],
  \end{empheq}
\end{subequations}

In practice, we assume that $T_y = L T_u$, where $L$ is an integer. In this case, we could denote the state vector based on the sampling rate $T_y$ by $\mathbf{z}[k] = \mathbf{x}[Lk]$. Then we have $\mathcal{A}_d = \mathcal{A}^L$, $\mathcal{B}_d = \begin{bmatrix}
  \mathcal{A}^{L-1} \mathcal{B} & \mathcal{A}^{L-2} \mathcal{B} & \cdots \mathcal{B}
\end{bmatrix}$, $\mathbf{u}_d[k] = \begin{bmatrix}
  \mathbf{u}[Lk] & \mathbf{u}[Lk+1] & \cdots \mathbf{u}[Lk+L-1]
\end{bmatrix}^T$. By this way, the discrete model is finally formulated as
\begin{subequations} \label{fml:the:state-dis}
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{empheq}[left=\empheqlbrace]{align}
    \mathbf{z} [k+1] &= \mathcal{A}_d \mathbf{z} [k] + \mathcal{B}_d \mathbf{u}_d [k], \\
    \mathbf{y} [k] &= \mathcal{C} \mathbf{z} [k],
  \end{empheq}
\end{subequations}

Since the system is casual, we have
\begin{align} \label{fml:the:out-dis}
  \mathbf{y}[k] = \mathcal{F}[k] \mathbf{z}_0 + \mathcal{D}[k] \mathbf{u},
\end{align}
where $\mathcal{F}[k] = \mathcal{C}\mathcal{A}_d^k$, $\mathcal{D}[k] = \mathcal{C} \begin{bmatrix}
  \mathcal{A}_d^{k-1} \mathcal{B}_d & \mathcal{A}_d^{k-2} \mathcal{B}_d & \cdots \mathcal{B}_d & \mathbf{0}_{N-kL}
\end{bmatrix}$ and \\$\mathbf{u} = \begin{bmatrix}
  \mathbf{u}_d[0] & \mathbf{u}_d[1] & \cdots & \mathbf{u}_d[M-1]
\end{bmatrix}^T$. The initial condition is configured as \\$\mathbf{z}_{0} = \mathbf{z}[0] = \begin{bmatrix}
  0 & y_1(0) & 0 & \frac{y_2(0)}{\alpha_2} & \cdots & 0 & \frac{y_{\chi}(0)}{\alpha_{\chi}}
\end{bmatrix}^T$.

Let $\mathbf{y} = \begin{bmatrix}
\mathbf{y}[1]^T & \mathbf{y}[2]^T & \cdots & \mathbf{y}[M]^T
\end{bmatrix}^T$, $\mathcal{F}_{\boldsymbol{\theta}} = \begin{bmatrix}
\mathcal{F}[0] & \mathcal{F}[1] & \cdots & \mathcal{F}[M-1]
\end{bmatrix}^T$ and \\$\mathcal{D}_{\boldsymbol{\theta}} = \begin{bmatrix}
\mathcal{D}[0] & \mathcal{D}[1] & \cdots & \mathcal{D}[M-1]
\end{bmatrix}^T$, we can represent the whole multichannel phasic component by
\begin{align} \label{fml:the:out-dis-all}
  \mathbf{y} = \mathcal{F}_{\boldsymbol{\theta}} \mathbf{z}_{0} + \mathcal{D}_{\boldsymbol{\theta}} \mathbf{u},
\end{align}
where we use $\boldsymbol{\theta}$ to denote the learnable vector $\begin{bmatrix}
  \tau_r & \tau_d & \alpha_1 & \alpha_2 & \cdots & \alpha_{\chi}
\end{bmatrix}^T$.

\subsection{Model the tonic component with spline functions}

The tonic signal of the $n^{\mathrm{th}}$ channel could be viewed as the coefficients $q_n (t)$ convolved with the cubic B-spline function $\psi(t)$,
\begin{align}
  s_n(t) =  \psi(t) \otimes q_n (t).
\end{align}

The coefficients could be viewed as a time-series signal composed of several spikes, i.e. $q_n (t) = \sum_{j=1}^N q_j \delta(t - jT_s)$, where $T_s$ is the sampling rate of the coefficients. $T_s$ could be used to control the smoothness of the B-spline function. To ensure the smoothness, we usually use a longer period for $T_s$ like 6s. With these configurations, we could discretize the coefficients as $\mathbf{q} = \begin{bmatrix}
  q_1 & q_2 & \cdots q_n
\end{bmatrix}$.
{\color{red}   $q_n(t)$ has also been used for the coefficient signal for nth channel.}
The convolution on $\psi(t)$ could be formulated by a Toeplitz matrix $\mathbf{C}$. The $k^{\mathrm{th}}$ row of the matrix could be formulated as
\begin{align}
  \mathbf{c}_k = \begin{bmatrix}
    \psi(kT_y + T_s) & \psi(kT_y) & \psi(kT_y - T_s) & \cdots & \psi(T_u - T_s)
  \end{bmatrix}.
\end{align}
With the discretion, we could formulate the tonic component as
\begin{align}
  \mathbf{s}_n = \mathbf{C} \mathbf{q}_n.
\end{align}
{\color{red} we should change $q_n$}

\subsection{Preprocess the data}

Preprocessing aims at removing the outliers in the raw data. The whole process is shown in \autoref{alg:preprocessing}. We find the peaks of the differentiated raw data. The searching method is based on comparing the 2 values near the center point. The data patches with outliers are all replaced by their spline approximations. Most of the outliers could be removed by this way.
\begin{algorithm}[tb]
  \caption{The preprocessing applied to the raw data.}
  \label{alg:preprocessing}
  \begin{algorithmic}[1]
    \REQUIRE The raw data $\{\mathbf{y}_{\mathrm{SC}_n}\}_{n=1}^\chi$ for $\chi$ channels.
    \ENSURE The preprocessed data $\{\hat{\mathbf{y}}_{\mathrm{SC}_n}\}_{n=1}^\chi$ for $\chi$ channels.
    % if-then-else
    \FOR{$n$ from $1$ to $\chi$}
      \STATE Find the local positive and negative peaks of the differential data $\dot{\mathbf{y}}_{\mathrm{SC}_n}$, where peaks means the data with a value larger than the around values by 0.1;
      \FORALL{founded peaks}
        \STATE Select 4 points near the peak, the range is limited in 100 points around the peak;
        \STATE Use the spline interpolation of the selected 4 points to replace the 100 points;
      \ENDFOR
      \STATE Perform the 64 order low-pass FIR filter, the cut-off frequency is 3 Hz;
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

\subsection{Solve the deconvolution}

After modeling the data by the aforementioned two methods, we could formulate the $n^{\mathrm{th}}$ channel of the SC data as follows,
\begin{align}
  \mathbf{y}_n = \mathcal{F}_{\boldsymbol{\theta}_n} \mathbf{z}_{0} + \mathcal{D}_{\boldsymbol{\theta}_n} \mathbf{u} + \mathbf{C} \mathbf{q}_n + \boldsymbol{\nu}_n,
\end{align}
where $\boldsymbol{\theta}_n = \begin{bmatrix}
\tau_r & \tau_d & \alpha_n
\end{bmatrix}^T$ is the subset of the learnable vector $\boldsymbol{\theta}$, $\mathbf{q}_n$ is the decomposition coefficients of the $n^{\mathrm{th}}$ tonic component and $\boldsymbol{\nu}_n$ is the noise vector for $n^{th}$ channel. {\color{red} (need more exact define of $\boldsymbol{\nu}$) }. For the phasic decomposition, the coefficients, i.e. the neural stimuli $\mathbf{u}$ is shared, and the model parameters are not totally shared crossing the channels. For the tonic decomposition, the modeling function $\mathbf{C}$ is shared crossing the channels, while the coefficients $\mathbf{q}_n$ are different among different channels.

The multichannel deconvolution with the tonic separation could be formulated by the following constrained joint optimization problem,
\begin{subequations} \label{fml:the:optimization}
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{align}
    \arg\min\limits_{\substack{\boldsymbol{\theta},~\mathbf{u}, \\ \{\mathbf{q}_n\}_{n=1}^{\chi}, \\ \lambda,~\mu_n}} & \frac{1}{\chi} \left( \sum_{n=1}^{\chi} \mathcal{J}(\boldsymbol{\theta}_n,~\mathbf{u},~\mathbf{q}_n) + \mu_n \lVert \mathbf{q}_n \rVert_2^2 \right) + \lambda \lVert \mathbf{u} \rVert^p_p, \\
    \mathrm{s.t.}~&\mathcal{J}(\boldsymbol{\theta}_n,~\mathbf{u},~\mathbf{q}_n) = \lVert \mathbf{y}_n - \mathcal{F}_{\boldsymbol{\theta}_n} \mathbf{z}_{0} - \mathcal{D}_{\boldsymbol{\theta}_n} \mathbf{u} - \mathbf{C} \mathbf{q}_n \rVert^2_2, \label{fml:loss-channel}\\
    & \boldsymbol{\Gamma} \boldsymbol{\theta} \preccurlyeq \mathbf{b},~ \mathbf{u} \succcurlyeq \mathbf{0} \label{fml:cons-phasic}\\
    & \forall~n,~\mathbf{C}\mathbf{q}_n \preccurlyeq \mathbf{y}_n, \label{fml:cons-tonic}
  \end{align}
\end{subequations}
where \eqref{fml:loss-channel} represents the loss function for the $n^{\mathrm{th}}$ channel, \eqref{fml:cons-phasic} is the constraint of the phasic decomposition, and \eqref{fml:cons-tonic} is the constraint of the tonic extraction. In \eqref{fml:cons-phasic}, the Tikhonov matrix $\boldsymbol{\Gamma}$ and the boundary vector $\mathbf{b}$ are defined as
\begin{subequations} \label{fml:the:constraint}
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{align}
    \boldsymbol{\Gamma} &= \begin{bmatrix}
      1 & -1 & 0 & 0 & \cdots & 0 & 0 \\
      0 & 0 & 1 & -1 & \cdots & 0 & 0 \\
      \vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
      0 & 0 & 0 & 0 & \cdots & 1 & -1
    \end{bmatrix}, \\
    \mathbf{b} &= \begin{bmatrix}
      1.4 & -0.1 & 6.0 & -1.5 & 100.0 & -0.01 & \cdots & 100.0 & -0.01
    \end{bmatrix}^T
  \end{align}
\end{subequations}

Conventionally, it is difficult to decide the proper the regularization coefficients $\lambda,~\mu_n$. In our work, we take the optimization of the coefficients into the consideration. The optimization is based on Focal Underdetermined System Solver (FOCUSS+) algorithm~\cite{murray2005visual}, while the estimation of the tunable regularization coefficients is based on  generalized cross-validation (GCV) method~\cite{zdunek2008improved}. 

The whole algorithm for the optimization is discussed in \autoref{alg:deconvolution}. The optimization could be divided into an outer loop and an inner loop. In the outer loop, we solve the modeling parameters $\boldsymbol{\theta}$. The deconvolution and with the tonic extraction is performed in the inner loop.
\begin{algorithm}[!tb]
  \caption{The preprocessing applied to the raw data.}
  \label{alg:deconvolution}
  \begin{algorithmic}[1]
    \REQUIRE The preprocessed multichannel data $\{\hat{\mathbf{y}}_{\mathrm{SC}_n}\}_{n=1}^\chi$, the decomposition coefficients $\mathbf{u}, \{\mathbf{q}_{n}\}_{n=1}^\chi$, and the initialized parameters $\boldsymbol{\theta}$, where $\tau_d \sim U(0.10,~1.4)$, $\tau_r \sim U(1.5,~6.0)$, $\alpha_n \sim U(0.01,~1.0)$, $\mathbf{u} \sim 0$, and $\mathbf{q}_n \sim \mathcal{N}(0.1, 0.02)$.
    % if-then-else
    \FOR {$j$ from $1$ to $30$}
      \STATE Let $\boldsymbol{\theta},~\{\mathbf{q}_{n}\}$ fixed, use FOCUSS+ to solve $\tilde{\mathbf{u}}^{(j)}$. Set $\mathbf{u} = \tilde{\mathbf{u}}^{(j)}$;
      \STATE Let $\mathbf{u},~\{\mathbf{q}_{n}\}$ fixed, solve $\tilde{\boldsymbol{\theta}}^{(j)}$ by the interior method. Set $\boldsymbol{\theta} = \tilde{\boldsymbol{\theta}}^{(j)}$;
      \STATE Let $\boldsymbol{\theta},~\mathbf{u}$ fixed, use FOCUSS+ to solve $\{\tilde{\mathbf{q}}_{n}\}^{(j)}$. Set $\{\mathbf{q}_{n}\} = \{\tilde{\mathbf{q}}_{n}\}^{(j)}$;
    \ENDFOR
    \STATE Initialize $\hat{\boldsymbol{\theta}}^{(0)}=\boldsymbol{\theta}$, $\hat{\mathbf{u}}^{(0)}=\mathbf{u}$, $\{\hat{\mathbf{q}}_{n}\}^{(0)}=\{\mathbf{q}_{n}\}$;
    \FOR{$i$ from $0$ until converge}
      \STATE Let $\boldsymbol{\theta},~\{\mathbf{q}_{n}\}$ fixed, solve $\hat{\mathbf{u}}^{(j)}$ by the following steps;
      \STATE Let $\hat{\lambda}^{(i)(0)} = 2 \times 10^{-3}$,
      \FOR{$m$ from $1$ until converge}
        \STATE Let $\lambda = \hat{\lambda}^{(i)(m-1)}$, $\boldsymbol{\theta},~\{\mathbf{q}_{n}\}$ fixed, use FOCUSS+ to solve $\hat{\mathbf{u}}^{(i)(m)}$. Set $\mathbf{u} = \hat{\mathbf{u}}^{(i)(m)}$;
        \STATE Let $\mathbf{u},~\boldsymbol{\theta}~\{\mathbf{q}_{n}\}$ fixed, use FOCUSS+ to solve $\hat{\lambda}^{(i)(m)}$. Set $\lambda = \hat{\lambda}^{(i)(m)}$;
      \ENDFOR
      \STATE Set $\hat{\mathbf{u}}^{(i)} = \hat{\mathbf{u}}^{(i)(m)}$.
      \STATE Let $\hat{\mu}^{(i)(0)} = 2 \times 10^{-3}$,
      \FOR{$m$ from $1$ until converge}
        \STATE Let $\mu = \hat{\mu}^{(i)(m-1)}$, $\boldsymbol{\theta},\mathbf{u}$ fixed, use FOCUSS+ to solve $\{\hat{\mathbf{q}}_{n}\}^{(i)(m)}$. Set $\{\mathbf{q}_{n}\} = \{\hat{\mathbf{q}}_{n}\}^{(i)(m)}$;
        \STATE Let $\mathbf{u},~\boldsymbol{\theta}~\{\mathbf{q}_{n}\}$ fixed, use FOCUSS+ to solve $\hat{\mu}^{(i)(m)}$. Set $\mu = \hat{\mu}^{(i)(m)}$;
      \ENDFOR
      \STATE Set $\{\hat{\mathbf{q}}_{n}\}^{(i)} =  \{\hat{\mathbf{q}}_{n}\}^{(i)(m)}$.
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

\begin{subequations} \label{fml:the:optimization2}
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{align}
  \argmin\limits_{\substack{\boldsymbol{\theta},~\mathbf{u}, \\ \{\mathbf{q}_n\}_{n=1}^{\chi}, \\ \lambda,~\{\mu_n\}_{n=1}^{\chi}}} & \frac{1}{\chi} \left( \sum_{n=1}^{\chi} \mathcal{J}_n + \mu_n \lVert \mathbf{q}_n \rVert_2^2 \right) + \lambda \lVert \mathbf{u} \rVert^p_p, \\
  \mathrm{s.t.}~&\mathcal{J}_n = \lVert \mathbf{y}_n - \mathcal{F}_{\boldsymbol{\theta}_n} \mathbf{z}_{0} - \mathcal{D}_{\boldsymbol{\theta}_n} \mathbf{u} - \mathbf{S} \mathbf{q}_n \rVert^2_2, \label{fml:loss-channel2}\\
  & \boldsymbol{\Gamma} \boldsymbol{\theta} \preccurlyeq \mathbf{b},~ \mathbf{u} \succcurlyeq \mathbf{0} \label{fml:cons-phasic2}\\
  & \forall~n,~\mathbf{q}_n \succcurlyeq \mathbf{0},~\mathbf{S}\mathbf{q}_n \preccurlyeq \mathbf{y}_n, \label{fml:cons-tonic2}
  \end{align}
\end{subequations}

$m^{\mathrm{th}}$ \qquad $\tilde{\mathbf{S}}$ \qquad $t^{(u)}_k$

\begin{align} \label{fml:tonic-basis-1}
\begin{aligned}
\tilde{\mathbf{s}}_m = \big[ &\psi(mT_y - t^{(u)}_1 - T_{ds}) & \psi(mT_y - t^{(u)}_2 - T_{ds}) && \cdots && \psi(mT_y - t^{(u)}_K - T_{ds}) \big].
\end{aligned}
\end{align}

\begin{align} \label{fml:tonic-basis-2}
\mathbf{R} = \begin{bmatrix}
1/M & 2/M & 3/M & 4/M & \cdots & 1 \\
1 & 1 & 1 & 1 & \cdots & 1 
\end{bmatrix}^T,
\end{align}

\begin{align}
\mathbf{s}_n = \mathbf{S} \mathbf{q}_n = \begin{bmatrix}
\tilde{\mathbf{S}} & \mathbf{R}
\end{bmatrix} \begin{bmatrix}
\tilde{\mathbf{q}}_n \\ r_{1n} \\ r_{0n}
\end{bmatrix}.
\end{align}

\begin{subequations} \label{fml:the:second-tonic}
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{align}
  \argmin\limits_{\mathbf{q}_n} & \lVert \mathbf{s}_n - \mathbf{S} \mathbf{q}_n \rVert^2_2 + \mu_n \lVert \mathbf{q}_n \rVert_2^2, \\
  \mathrm{s.t.}~& \mathbf{s}_n = \mathbf{y}_n - \mathcal{F}_{\boldsymbol{\theta}_n} \mathbf{z}_{0} - \mathcal{D}_{\boldsymbol{\theta}_n} \mathbf{u}, \label{fml:the:second-tonic-obj}\\
  & \forall~n,~\mathbf{q}_n \succcurlyeq \mathbf{0},~\mathbf{S}\mathbf{q}_n \preccurlyeq \mathbf{y}_n,
  \end{align}
\end{subequations}

\begin{subequations} \label{fml:the:second-phasic}
  \renewcommand{\theequation}
  {\theparentequation-\arabic{equation}}
  \begin{align}
  \argmin\limits_{\boldsymbol{\theta},~\mathbf{u}} & \frac{1}{\chi}  \left( \sum_{n=1}^{\chi} \lVert \mathbf{p}_n - \mathcal{F}_{\boldsymbol{\theta}_n} \mathbf{z}_{0} - \mathcal{D}_{\boldsymbol{\theta}_n} \rVert^2_2 \right) + \lambda \lVert \mathbf{u} \rVert_p^p, \\
  \mathrm{s.t.}~& \mathbf{p}_n = \mathbf{y}_n -  \mathbf{S} \mathbf{q}_n, \\
  & \boldsymbol{\Gamma} \boldsymbol{\theta} \preccurlyeq \mathbf{b},~ \mathbf{u} \succcurlyeq \mathbf{0}.
  \end{align}
\end{subequations}

\bibliographystyle{ieeetr}
\bibliography{ref}

\end{document}
